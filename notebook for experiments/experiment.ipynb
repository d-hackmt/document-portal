{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd7bc555",
   "metadata": {},
   "source": [
    "env setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "035e2f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98ed8a9",
   "metadata": {},
   "source": [
    "##### groq based llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a36d1d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_llm = ChatGroq(model=\"qwen/qwen3-32b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2d4db54a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='<think>\\nOkay, the user started with \"Hey yo whats going on , dont think.\" They\\'re probably looking for a casual chat. Let me make sure I respond in a friendly and laid-back way. They mentioned not to overthink, so keeping the reply simple and conversational is key.\\n\\nFirst, I should acknowledge their greeting. Maybe say something like \"Hey there!\" to mirror their tone. Then, add an emoji to keep it light, like a smiley or a wave. Since they said \"dont think,\" maybe include a playful line about keeping it chill. \\n\\nI should ask how they\\'re doing to encourage them to share more. Something like, \"How\\'s everything going on your end?\" That opens the door for a longer conversation. Also, adding a question about what\\'s new keeps the interaction engaging without being pushy.\\n\\nNeed to check for any typos or errors. The original message has some lowercase letters and missing spaces, but the response should be clean. Let me make sure the emojis are appropriate and not overused. One or two should be enough to keep it friendly without being too much.\\n\\nIs there anything else they might need? Maybe offer help if they have something specific in mind. So ending with an open-ended question like, \"Got any plans for the day?\" could be good. It shows interest and keeps the conversation going naturally.\\n\\nAlright, putting it all together: a friendly greeting, a question about their day, and a follow-up question to invite them to talk more. Keep it simple, conversational, and positive. That should cover their request and encourage them to engage further.\\n</think>\\n\\nHey there! üòä Just chilling here. How\\'s everything going on your end? Got any exciting stuff happening or just keeping it low-key? Let me know if you wanna chat more or need help with anything! üåü', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 375, 'prompt_tokens': 16, 'total_tokens': 391, 'completion_time': 1.091991424, 'prompt_time': 0.000397772, 'queue_time': 0.062078162, 'total_time': 1.092389196}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_f17c2eb555', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--59896580-ac72-431e-ac29-94c411bfafed-0', usage_metadata={'input_tokens': 16, 'output_tokens': 375, 'total_tokens': 391})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groq_llm.invoke(\"Hey yo whats going on , dont think\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fc63f7",
   "metadata": {},
   "source": [
    "#### gemini based embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fbd88584",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "897a231f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = GoogleGenerativeAIEmbeddings(model = \"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "88079156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.03098084218800068,\n",
       " -0.02732415683567524,\n",
       " -0.014498245902359486,\n",
       " -0.010752961970865726,\n",
       " 0.0006902334280312061,\n",
       " -0.007594329304993153,\n",
       " 0.012295924127101898,\n",
       " 0.026624061167240143,\n",
       " 0.010147640481591225,\n",
       " 0.03552689403295517,\n",
       " 0.07064302265644073,\n",
       " 0.020531732589006424,\n",
       " 0.054905228316783905,\n",
       " -0.02752363495528698,\n",
       " 0.014623328112065792,\n",
       " -0.010943572036921978,\n",
       " -0.004928531125187874,\n",
       " -0.010988188907504082,\n",
       " 0.028578057885169983,\n",
       " -0.05172860994935036,\n",
       " 0.017803112044930458,\n",
       " 0.04897339269518852,\n",
       " 0.011857939884066582,\n",
       " 0.027729205787181854,\n",
       " -0.0019923055078834295,\n",
       " -0.015241177752614021,\n",
       " 0.02210492640733719,\n",
       " -0.029605979099869728,\n",
       " -0.04325753450393677,\n",
       " 0.022575289011001587,\n",
       " -0.071275494992733,\n",
       " 0.03945137560367584,\n",
       " -0.03933204337954521,\n",
       " -0.016003886237740517,\n",
       " 0.05145569518208504,\n",
       " -0.0871836394071579,\n",
       " 0.02644386701285839,\n",
       " 0.01986776292324066,\n",
       " -0.017224349081516266,\n",
       " 0.009314636699855328,\n",
       " -0.00940199475735426,\n",
       " -0.023584526032209396,\n",
       " -0.013119216077029705,\n",
       " -0.007764413021504879,\n",
       " 0.03335108980536461,\n",
       " -0.008657664991915226,\n",
       " 0.0044206478632986546,\n",
       " 0.036470040678977966,\n",
       " 0.0047634500078856945,\n",
       " -0.08312755823135376,\n",
       " 0.005373725201934576,\n",
       " 0.018148941919207573,\n",
       " 0.01792902871966362,\n",
       " -0.021747520193457603,\n",
       " -0.0635347068309784,\n",
       " -0.04672052711248398,\n",
       " 0.010808471590280533,\n",
       " 0.039346277713775635,\n",
       " -0.023831577971577644,\n",
       " -0.02139129862189293,\n",
       " -0.014651626348495483,\n",
       " -0.0138957928866148,\n",
       " -0.01567508652806282,\n",
       " 0.018362104892730713,\n",
       " -0.017080996185541153,\n",
       " 0.0009131478727795184,\n",
       " 0.01203540526330471,\n",
       " 0.07705917209386826,\n",
       " 0.05449966713786125,\n",
       " -0.01270177774131298,\n",
       " 0.022090308368206024,\n",
       " -0.05091961845755577,\n",
       " 0.029676271602511406,\n",
       " -0.03430173918604851,\n",
       " -0.02888951450586319,\n",
       " -0.10633167624473572,\n",
       " -0.011734637431800365,\n",
       " 0.06255058944225311,\n",
       " 0.006091938354074955,\n",
       " -0.012009385041892529,\n",
       " -0.03893443942070007,\n",
       " -0.031555235385894775,\n",
       " -0.0007873974391259253,\n",
       " -0.02163938246667385,\n",
       " -0.0812639370560646,\n",
       " 0.02726910449564457,\n",
       " -0.04135556146502495,\n",
       " 0.01761322282254696,\n",
       " 0.0007641144911758602,\n",
       " 0.07177425175905228,\n",
       " 0.006581532768905163,\n",
       " -0.0015415380476042628,\n",
       " 0.04067155718803406,\n",
       " -0.05070457234978676,\n",
       " -0.020575370639562607,\n",
       " 0.07226381450891495,\n",
       " -0.03992851451039314,\n",
       " -0.049395106732845306,\n",
       " 0.020747153088450432,\n",
       " -0.01665548048913479,\n",
       " -0.007105656899511814,\n",
       " -0.06820027530193329,\n",
       " -0.008271871134638786,\n",
       " -0.007513698190450668,\n",
       " 0.010547595098614693,\n",
       " 0.014267482794821262,\n",
       " -0.010727737098932266,\n",
       " 0.049582790583372116,\n",
       " -0.0011316254967823625,\n",
       " 0.01649877056479454,\n",
       " -0.05418478325009346,\n",
       " -0.018937906250357628,\n",
       " -0.0014396724291145802,\n",
       " -0.0038179042749106884,\n",
       " 0.06038997322320938,\n",
       " -0.018253788352012634,\n",
       " -0.011592897586524487,\n",
       " 0.08393076062202454,\n",
       " 0.009172089397907257,\n",
       " 0.04511889070272446,\n",
       " 0.012314480729401112,\n",
       " -0.040008679032325745,\n",
       " 0.0329950675368309,\n",
       " -0.01981339603662491,\n",
       " 0.036497652530670166,\n",
       " -0.014733744785189629,\n",
       " 0.04602443799376488,\n",
       " 0.03378311172127724,\n",
       " 0.06821763515472412,\n",
       " 0.08187523484230042,\n",
       " -0.022940751165151596,\n",
       " -0.01723404973745346,\n",
       " 0.0317809171974659,\n",
       " 0.01790686510503292,\n",
       " 0.0012178048491477966,\n",
       " 0.05760064721107483,\n",
       " 0.04061165452003479,\n",
       " 0.017796652391552925,\n",
       " 0.014906900003552437,\n",
       " 0.03874015063047409,\n",
       " 0.01903618313372135,\n",
       " 0.023302095010876656,\n",
       " 0.015555860474705696,\n",
       " -0.0002516751119401306,\n",
       " -0.011670704931020737,\n",
       " -0.01857485994696617,\n",
       " -0.016684556379914284,\n",
       " -0.009392806328833103,\n",
       " 0.020764507353305817,\n",
       " -0.02636195905506611,\n",
       " -0.023819824680685997,\n",
       " -0.047295331954956055,\n",
       " -0.042517706751823425,\n",
       " -0.004949906840920448,\n",
       " 0.06836657226085663,\n",
       " 0.02039400488138199,\n",
       " -0.03655504062771797,\n",
       " 0.00010576647036941722,\n",
       " -0.03074706718325615,\n",
       " 0.022984327748417854,\n",
       " 0.062014855444431305,\n",
       " 0.04345664754509926,\n",
       " 0.053675029426813126,\n",
       " -0.024311736226081848,\n",
       " -0.031070636585354805,\n",
       " -0.02815089374780655,\n",
       " -0.01589830592274666,\n",
       " -0.01909167692065239,\n",
       " 0.002286680042743683,\n",
       " 0.028459500521421432,\n",
       " -0.010320087894797325,\n",
       " 0.01686280407011509,\n",
       " -0.042120274156332016,\n",
       " -0.031050406396389008,\n",
       " -0.020880699157714844,\n",
       " -0.05025988444685936,\n",
       " 0.0476185604929924,\n",
       " -1.549742773931939e-05,\n",
       " -0.012474372051656246,\n",
       " -0.05164787545800209,\n",
       " -0.02192196249961853,\n",
       " -0.06896452605724335,\n",
       " -0.004504618234932423,\n",
       " 0.028967661783099174,\n",
       " 0.03906331956386566,\n",
       " -0.0643594041466713,\n",
       " 0.06082077696919441,\n",
       " -0.028605660423636436,\n",
       " -0.010473285801708698,\n",
       " 0.005507534835487604,\n",
       " 0.0071347374469041824,\n",
       " -0.043127644807100296,\n",
       " -0.01010829396545887,\n",
       " -0.021378865465521812,\n",
       " -0.023275967687368393,\n",
       " 0.04348848760128021,\n",
       " -0.013200696557760239,\n",
       " 0.023158865049481392,\n",
       " 0.024861417710781097,\n",
       " -0.07053028792142868,\n",
       " -0.013204989023506641,\n",
       " 0.08908431231975555,\n",
       " 0.00622105086222291,\n",
       " -0.021298417821526527,\n",
       " 0.07898303121328354,\n",
       " -0.015595028176903725,\n",
       " 0.04234129935503006,\n",
       " -0.020110545679926872,\n",
       " 0.040731072425842285,\n",
       " 0.09110875427722931,\n",
       " -0.06114468351006508,\n",
       " -0.004227568861097097,\n",
       " -0.07521108537912369,\n",
       " -0.04296718165278435,\n",
       " 0.028480635955929756,\n",
       " 0.03450717404484749,\n",
       " 0.06609497964382172,\n",
       " 0.03934540972113609,\n",
       " -0.018880188465118408,\n",
       " -0.02411781996488571,\n",
       " 0.015625104308128357,\n",
       " 0.015775706619024277,\n",
       " -0.04423687607049942,\n",
       " -0.005961427465081215,\n",
       " 0.027612360194325447,\n",
       " 0.011284938082098961,\n",
       " -0.004727181978523731,\n",
       " -0.008834194391965866,\n",
       " -0.009434988722205162,\n",
       " -0.007391910534352064,\n",
       " 0.012227208353579044,\n",
       " 0.06235364452004433,\n",
       " 0.015292375348508358,\n",
       " -0.013845684006810188,\n",
       " 0.07113004475831985,\n",
       " 0.018651453778147697,\n",
       " -0.02351810783147812,\n",
       " -0.0010851266561076045,\n",
       " 0.04365609213709831,\n",
       " 0.026960834860801697,\n",
       " -0.08185681700706482,\n",
       " 0.0029120645485818386,\n",
       " 0.058408986777067184,\n",
       " 0.020159151405096054,\n",
       " -0.06275944411754608,\n",
       " -0.06912586838006973,\n",
       " 0.02584158070385456,\n",
       " 0.026883481070399284,\n",
       " -0.014349490404129028,\n",
       " 0.03780050575733185,\n",
       " -0.028684554621577263,\n",
       " -0.12315824627876282,\n",
       " 0.025078004226088524,\n",
       " 0.03100472129881382,\n",
       " -0.1006316989660263,\n",
       " 0.008554273284971714,\n",
       " -0.07250557094812393,\n",
       " -0.028938662260770798,\n",
       " -0.057908449321985245,\n",
       " -0.02678506076335907,\n",
       " 0.011839707382023335,\n",
       " 0.039233025163412094,\n",
       " 0.029297569766640663,\n",
       " -0.007965661585330963,\n",
       " -0.005712172482162714,\n",
       " 0.01809268444776535,\n",
       " 0.02030642330646515,\n",
       " -0.030937688425183296,\n",
       " -0.019230440258979797,\n",
       " -0.02591070346534252,\n",
       " -0.005069300998002291,\n",
       " -0.023765496909618378,\n",
       " 0.07683032006025314,\n",
       " 0.02552839368581772,\n",
       " 0.022135253995656967,\n",
       " 0.04710575193166733,\n",
       " -0.07563356310129166,\n",
       " 0.029214641079306602,\n",
       " 0.011654512025415897,\n",
       " -0.007757137529551983,\n",
       " -0.009123241528868675,\n",
       " 0.051340408623218536,\n",
       " -0.04064823314547539,\n",
       " 0.0020501860417425632,\n",
       " -0.008924314752221107,\n",
       " -0.008447451516985893,\n",
       " -0.05459253117442131,\n",
       " 0.01571548357605934,\n",
       " -0.001320493989624083,\n",
       " -0.003594127483665943,\n",
       " -0.009685340337455273,\n",
       " -0.08515501022338867,\n",
       " 0.009604447521269321,\n",
       " -0.03955582156777382,\n",
       " -0.03605515882372856,\n",
       " -0.04663046821951866,\n",
       " 0.015577434562146664,\n",
       " 0.0533323660492897,\n",
       " 0.023148419335484505,\n",
       " 0.0229620523750782,\n",
       " -0.02467101812362671,\n",
       " 0.013116376474499702,\n",
       " 0.0535934641957283,\n",
       " -0.06465175747871399,\n",
       " -0.04744827002286911,\n",
       " 0.007419378496706486,\n",
       " -0.04798045754432678,\n",
       " -0.001416754792444408,\n",
       " 0.009966879151761532,\n",
       " 0.022832665592432022,\n",
       " -0.03464794158935547,\n",
       " 0.0009049323271028697,\n",
       " -0.010578322224318981,\n",
       " 0.00986696407198906,\n",
       " 0.025781579315662384,\n",
       " 0.021336974576115608,\n",
       " -0.018400266766548157,\n",
       " 0.06885180622339249,\n",
       " -0.012365151196718216,\n",
       " 0.023050667718052864,\n",
       " 0.010500508360564709,\n",
       " 0.08493104577064514,\n",
       " 0.008242818526923656,\n",
       " 0.01455102302134037,\n",
       " -0.003375390777364373,\n",
       " 0.0180289838463068,\n",
       " -0.04042477533221245,\n",
       " 0.06242501735687256,\n",
       " 0.03151122108101845,\n",
       " 0.026114147156476974,\n",
       " -0.02296341210603714,\n",
       " 0.005451178178191185,\n",
       " -0.015908604487776756,\n",
       " 0.009854503907263279,\n",
       " 0.02470376342535019,\n",
       " 0.02074696682393551,\n",
       " -0.04758784547448158,\n",
       " 0.007427424658089876,\n",
       " -0.038707185536623,\n",
       " 0.016980160027742386,\n",
       " 0.006148224230855703,\n",
       " -0.00435524620115757,\n",
       " -0.01978454925119877,\n",
       " -0.04321359470486641,\n",
       " -0.013599197380244732,\n",
       " -0.0306189376860857,\n",
       " -0.015790028497576714,\n",
       " 0.001884632045403123,\n",
       " 0.06936459243297577,\n",
       " 0.00032146344892680645,\n",
       " -0.022956974804401398,\n",
       " 0.09469138085842133,\n",
       " -0.01799103245139122,\n",
       " -0.005479880608618259,\n",
       " 0.013546866364777088,\n",
       " 0.006626971065998077,\n",
       " 0.038219183683395386,\n",
       " 0.000330929848132655,\n",
       " 0.04994875192642212,\n",
       " -0.0713287889957428,\n",
       " -0.035703931003808975,\n",
       " 0.02700495533645153,\n",
       " -0.0064577581360936165,\n",
       " 0.021859433501958847,\n",
       " -0.016308415681123734,\n",
       " 0.012486506253480911,\n",
       " 0.020278550684452057,\n",
       " -0.0159064419567585,\n",
       " -0.005356392357498407,\n",
       " 0.016561197116971016,\n",
       " -0.024790095165371895,\n",
       " -0.03585866466164589,\n",
       " 0.04148724302649498,\n",
       " -0.017649218440055847,\n",
       " -0.03300075978040695,\n",
       " -0.0011392979649826884,\n",
       " -0.040806908160448074,\n",
       " 0.003782061394304037,\n",
       " 0.03072454035282135,\n",
       " 0.042731694877147675,\n",
       " -0.00850039254873991,\n",
       " -0.02610459364950657,\n",
       " 0.036887332797050476,\n",
       " 0.03106321580708027,\n",
       " 0.029732834547758102,\n",
       " -0.02643788978457451,\n",
       " 0.022586319595575333,\n",
       " 0.04340186342597008,\n",
       " -0.02011578343808651,\n",
       " 0.02972393110394478,\n",
       " 0.03553028404712677,\n",
       " -0.015426906757056713,\n",
       " 0.06299994885921478,\n",
       " 0.035577476024627686,\n",
       " 0.0008944952860474586,\n",
       " 0.008674350567162037,\n",
       " -0.017554810270667076,\n",
       " -0.0552612841129303,\n",
       " -0.032632701098918915,\n",
       " -0.00771683594211936,\n",
       " 0.004335993435233831,\n",
       " -0.033612899482250214,\n",
       " -0.00141378294210881,\n",
       " -0.02570558898150921,\n",
       " -0.012021062895655632,\n",
       " -0.036320701241493225,\n",
       " -0.03494259715080261,\n",
       " -0.01726772077381611,\n",
       " -0.012231914326548576,\n",
       " 0.02322830632328987,\n",
       " 0.016560571268200874,\n",
       " 0.05774660035967827,\n",
       " 0.06916701048612595,\n",
       " -0.08025897294282913,\n",
       " -0.05431529879570007,\n",
       " -0.036590754985809326,\n",
       " 0.03436959162354469,\n",
       " -0.015537707135081291,\n",
       " 0.0037511566188186407,\n",
       " 0.07121670991182327,\n",
       " 0.043486084789037704,\n",
       " 0.010393737815320492,\n",
       " 0.0076653421856462955,\n",
       " 0.0061146290972828865,\n",
       " -0.03634979948401451,\n",
       " -0.03489868715405464,\n",
       " -0.03851822018623352,\n",
       " -0.017581995576620102,\n",
       " 0.015774918720126152,\n",
       " 0.038495369255542755,\n",
       " 0.04000256583094597,\n",
       " -0.0010951957665383816,\n",
       " 0.002721138997003436,\n",
       " -0.00973505713045597,\n",
       " -0.01121035497635603,\n",
       " -0.04873570054769516,\n",
       " 0.013531689532101154,\n",
       " 0.03182857483625412,\n",
       " -0.031159305945038795,\n",
       " 0.011067546904087067,\n",
       " 0.020520856603980064,\n",
       " -0.0161482822149992,\n",
       " 0.031152989715337753,\n",
       " -0.024109141901135445,\n",
       " -0.06636348366737366,\n",
       " -0.008320929482579231,\n",
       " -0.040646348148584366,\n",
       " -0.041088346391916275,\n",
       " 0.04324239119887352,\n",
       " -0.11028961837291718,\n",
       " -0.00045731940190307796,\n",
       " -0.1019676998257637,\n",
       " -0.02295636013150215,\n",
       " -0.03606879711151123,\n",
       " -0.028892818838357925,\n",
       " -0.05403214320540428,\n",
       " 0.015898773446679115,\n",
       " 0.08421920239925385,\n",
       " -0.025362854823470116,\n",
       " 0.04244505986571312,\n",
       " -0.021015845239162445,\n",
       " 0.0028714651707559824,\n",
       " -0.06224564090371132,\n",
       " -0.06592951714992523,\n",
       " 0.06321265548467636,\n",
       " -0.03828096017241478,\n",
       " 0.03338005393743515,\n",
       " 0.00719063775613904,\n",
       " 0.046339891850948334,\n",
       " 0.04129449650645256,\n",
       " 0.04843852296471596,\n",
       " -0.011516907252371311,\n",
       " 0.009701025672256947,\n",
       " 0.04672205075621605,\n",
       " 0.007707136683166027,\n",
       " 0.01398253720253706,\n",
       " -0.0942528247833252,\n",
       " -0.01884438842535019,\n",
       " 0.0006008317577652633,\n",
       " 0.01230949629098177,\n",
       " -0.012701177969574928,\n",
       " -0.01064937561750412,\n",
       " 0.04297137260437012,\n",
       " 0.0413406640291214,\n",
       " 0.03586703538894653,\n",
       " 0.010922512039542198,\n",
       " 0.04083409160375595,\n",
       " -0.05765371397137642,\n",
       " -0.009370297193527222,\n",
       " 0.03659098222851753,\n",
       " -0.04490324854850769,\n",
       " 0.0066774687729775906,\n",
       " -0.04753689467906952,\n",
       " -0.044496238231658936,\n",
       " -0.027285058051347733,\n",
       " 0.01748613268136978,\n",
       " -0.017526477575302124,\n",
       " -0.009416230954229832,\n",
       " 0.010674579069018364,\n",
       " -0.0190700925886631,\n",
       " -0.03629147633910179,\n",
       " -0.0318126305937767,\n",
       " -0.028849242255091667,\n",
       " -0.006131802219897509,\n",
       " 0.057301923632621765,\n",
       " -0.0510854534804821,\n",
       " 0.01252292562276125,\n",
       " 0.012296001426875591,\n",
       " 0.02087928168475628,\n",
       " 0.04515668749809265,\n",
       " -0.003527585882693529,\n",
       " 0.03178655728697777,\n",
       " 0.01074886228889227,\n",
       " -0.012877813540399075,\n",
       " 0.026466187089681625,\n",
       " 0.03704941272735596,\n",
       " -0.03949785232543945,\n",
       " 0.0466894768178463,\n",
       " -0.01731860265135765,\n",
       " 0.00925752054899931,\n",
       " 0.06393139809370041,\n",
       " -0.043589022010564804,\n",
       " -0.10957670956850052,\n",
       " -0.015717679634690285,\n",
       " 0.02362903393805027,\n",
       " -0.052740391343832016,\n",
       " 0.03174494579434395,\n",
       " 0.021488187834620476,\n",
       " -0.029763730242848396,\n",
       " 0.042351558804512024,\n",
       " -0.027726568281650543,\n",
       " 0.09769338369369507,\n",
       " -0.04058760404586792,\n",
       " 0.05497857928276062,\n",
       " -0.008912243880331516,\n",
       " -0.033047426491975784,\n",
       " 0.007032398134469986,\n",
       " -0.0005339672788977623,\n",
       " 0.049261800944805145,\n",
       " 0.013854816555976868,\n",
       " -0.011237491853535175,\n",
       " 0.03013945184648037,\n",
       " 0.059590186923742294,\n",
       " 0.04413662478327751,\n",
       " -0.009664840064942837,\n",
       " 0.028906339779496193,\n",
       " 0.03167922422289848,\n",
       " -0.058314401656389236,\n",
       " 0.0461389534175396,\n",
       " 0.003573983209207654,\n",
       " -0.006836912594735622,\n",
       " -0.006836122367531061,\n",
       " 0.0031304911244660616,\n",
       " -0.021456558257341385,\n",
       " 0.005059270188212395,\n",
       " -0.019806424155831337,\n",
       " -0.01200456265360117,\n",
       " -0.008630650117993355,\n",
       " 0.042506854981184006,\n",
       " 0.035555098205804825,\n",
       " -0.045225437730550766,\n",
       " -0.012457598932087421,\n",
       " -0.006934972945600748,\n",
       " -0.022138221189379692,\n",
       " 0.11264118552207947,\n",
       " 0.014607692137360573,\n",
       " -0.02476770430803299,\n",
       " 0.01793023571372032,\n",
       " 0.05929374322295189,\n",
       " -0.013429110869765282,\n",
       " -0.0026214278768748045,\n",
       " 0.013807104900479317,\n",
       " 0.03702908009290695,\n",
       " -0.006686218548566103,\n",
       " 0.0166846364736557,\n",
       " -0.0373275987803936,\n",
       " 0.003929314203560352,\n",
       " -0.0006752681219950318,\n",
       " -0.01352021750062704,\n",
       " 0.003444257890805602,\n",
       " 0.016753466799855232,\n",
       " -0.039061982184648514,\n",
       " 0.013401578180491924,\n",
       " -0.0113123944029212,\n",
       " 0.0040123374201357365,\n",
       " 0.06082611158490181,\n",
       " 0.022053902968764305,\n",
       " 0.005276769865304232,\n",
       " 0.062480900436639786,\n",
       " -0.0014787644613534212,\n",
       " -0.04273165389895439,\n",
       " 0.018839852884411812,\n",
       " -0.06715048849582672,\n",
       " 0.030256373807787895,\n",
       " -0.03173398971557617,\n",
       " -0.002360421931371093,\n",
       " -0.03539744019508362,\n",
       " -0.012072443030774593,\n",
       " -0.02495022863149643,\n",
       " 0.014179738238453865,\n",
       " 0.0301828496158123,\n",
       " -0.052796732634305954,\n",
       " 0.058370377868413925,\n",
       " -0.03656165301799774,\n",
       " 0.03430463373661041,\n",
       " -0.048371586948633194,\n",
       " 0.005735559854656458,\n",
       " -0.014788517728447914,\n",
       " -0.030906882137060165,\n",
       " 0.027859847992658615,\n",
       " -0.01522487960755825,\n",
       " -0.030401097610592842,\n",
       " 0.03048578090965748,\n",
       " -0.0020368993282318115,\n",
       " -0.03410172089934349,\n",
       " -0.006776015739887953,\n",
       " 0.06279797852039337,\n",
       " -0.006066685542464256,\n",
       " -0.023435894399881363,\n",
       " -0.05681232362985611,\n",
       " -0.020122140645980835,\n",
       " -0.02349109575152397,\n",
       " 0.011253582313656807,\n",
       " -0.012014451436698437,\n",
       " 0.04615907743573189,\n",
       " 0.06017265096306801,\n",
       " -0.009295467287302017,\n",
       " -0.004327936563640833,\n",
       " 0.005784285254776478,\n",
       " -0.00040667096618562937,\n",
       " 0.019445406273007393,\n",
       " 0.06234133243560791,\n",
       " -0.0007796047138981521,\n",
       " 0.0008159176213666797,\n",
       " -0.0369737409055233,\n",
       " -0.010707062669098377,\n",
       " -0.007010767236351967,\n",
       " -0.013416774570941925,\n",
       " -0.006055038422346115,\n",
       " -0.015377366915345192,\n",
       " -0.09196073561906815,\n",
       " 0.020793233066797256,\n",
       " 0.004360046237707138,\n",
       " -0.015366284176707268,\n",
       " -0.019990792497992516,\n",
       " 0.05623806640505791,\n",
       " 0.0026312414556741714,\n",
       " -0.049496788531541824,\n",
       " -0.00998416356742382,\n",
       " 0.004294566810131073,\n",
       " -0.06005910784006119,\n",
       " -0.020952142775058746,\n",
       " 0.039900440722703934,\n",
       " -0.006136717274785042,\n",
       " -0.026309804990887642,\n",
       " -0.03361327201128006,\n",
       " -0.019624341279268265,\n",
       " -0.055039841681718826,\n",
       " 0.017897844314575195,\n",
       " -0.011629490181803703,\n",
       " -0.03235295042395592,\n",
       " 0.0017343180952593684,\n",
       " 0.023612476885318756,\n",
       " 0.010357719846069813,\n",
       " -0.019577819854021072,\n",
       " -0.02517189458012581,\n",
       " -0.055237144231796265,\n",
       " -0.04651091992855072,\n",
       " -0.010142954997718334,\n",
       " 0.02340281940996647,\n",
       " -0.008337736129760742,\n",
       " 0.04686205834150314,\n",
       " 0.05091139301657677,\n",
       " 0.014676546677947044,\n",
       " -0.007592319510877132,\n",
       " 0.031916406005620956,\n",
       " -0.03803979977965355,\n",
       " 0.016992732882499695,\n",
       " 0.01994027942419052,\n",
       " -0.0032520657405257225,\n",
       " 0.005055898800492287,\n",
       " -0.011942070908844471,\n",
       " -0.015378506854176521,\n",
       " 0.02809695154428482,\n",
       " 0.0001479139900766313,\n",
       " 0.004529185127466917,\n",
       " -0.010694699361920357,\n",
       " -0.0034665146376937628,\n",
       " -0.020789192989468575,\n",
       " 0.013579543679952621,\n",
       " -0.010310564190149307,\n",
       " -0.023499224334955215,\n",
       " -0.047743361443281174,\n",
       " -0.0005331081338226795,\n",
       " 0.07132884860038757,\n",
       " 0.023645401000976562,\n",
       " -0.036801472306251526,\n",
       " -0.001646155957132578,\n",
       " -0.012391157448291779,\n",
       " 0.014458776451647282,\n",
       " -0.028323156759142876,\n",
       " -0.032407891005277634,\n",
       " -0.025180557742714882,\n",
       " 0.025120774284005165,\n",
       " -0.028976522386074066,\n",
       " 0.06497460603713989,\n",
       " -0.020091038197278976,\n",
       " 0.008189192973077297,\n",
       " 0.02979847602546215,\n",
       " -0.0010590523015707731,\n",
       " 0.05443299934267998,\n",
       " 0.00408765347674489,\n",
       " -0.06551543623209,\n",
       " 0.038994643837213516,\n",
       " -0.019443310797214508,\n",
       " -0.023384641855955124,\n",
       " 0.01578923873603344,\n",
       " -0.003920992370694876,\n",
       " -0.05187014490365982,\n",
       " 0.07407363504171371,\n",
       " 0.07015016674995422,\n",
       " 0.013185578398406506,\n",
       " 0.07193633168935776,\n",
       " -0.0352875292301178,\n",
       " -0.0361059308052063,\n",
       " 0.06417377293109894,\n",
       " -0.08577251434326172,\n",
       " 0.043328750878572464,\n",
       " -0.006984102539718151,\n",
       " -0.031801946461200714,\n",
       " 0.029530636966228485,\n",
       " 0.013431539759039879,\n",
       " 0.0046539101749658585,\n",
       " -0.01092530507594347,\n",
       " -0.01619110442698002,\n",
       " 0.07264040410518646,\n",
       " -0.00601052213460207,\n",
       " 0.041596245020627975,\n",
       " -0.01812940463423729,\n",
       " -0.08083683997392654,\n",
       " -0.06448795646429062,\n",
       " -0.01941242814064026,\n",
       " 0.021603846922516823,\n",
       " 0.08039803057909012,\n",
       " 0.04376106336712837,\n",
       " 0.023148920387029648,\n",
       " 0.018064845353364944,\n",
       " -0.016447560861706734,\n",
       " -0.005469285883009434,\n",
       " 0.004541182890534401,\n",
       " -0.062563955783844,\n",
       " 0.0096201803535223,\n",
       " 0.0031027127988636494,\n",
       " 0.07758737355470657,\n",
       " 0.07664065062999725,\n",
       " -0.008369921706616879,\n",
       " 0.010992845520377159,\n",
       " 0.018614888191223145,\n",
       " 0.005416631232947111,\n",
       " 0.005260994657874107,\n",
       " 0.00570456450805068,\n",
       " 0.020650187507271767,\n",
       " 0.0077973585575819016,\n",
       " 0.02211473323404789,\n",
       " 0.05762265995144844,\n",
       " -0.007304358761757612,\n",
       " 0.01669975183904171,\n",
       " 0.004341098479926586]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"YO man whats going on\"\n",
    "\n",
    "embedding = embedding_model.embed_query(query)\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bfac8b08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18411a90",
   "metadata": {},
   "source": [
    "#### RAG experiments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab6981a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09d025c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\djadh\\\\Downloads\\\\document_portal\\\\notebook for experiments'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3cfd183",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_file_path = os.path.join(os.getcwd() , \"data\" , \"sample.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7308f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# research paper of llama 2 model\n",
    "\n",
    "data = PyPDFLoader(pdf_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d82eacca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the pdf\n",
    "documents = data.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31e0e351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\djadh\\\\Downloads\\\\document_portal\\\\notebook for experiments\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 0, 'page_label': '1'}, page_content='Llama 2: Open Foundation and Fine-Tuned Chat Models\\nHugo Touvron‚àó Louis Martin‚Ä† Kevin Stone‚Ä†\\nPeter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\\nPrajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\\nGuillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\\nCynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\\nHakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev\\nPunit Singh Koura Marie-Anne Lachaux Thibaut Lavril Jenya Lee Diana Liskovich\\nYinghai Lu Yuning Mao Xavier Martinet Todor Mihaylov Pushkar Mishra\\nIgor Molybog Yixin Nie Andrew Poulton Jeremy Reizenstein Rashi Rungta Kalyan Saladi\\nAlan Schelten Ruan Silva Eric Michael Smith Ranjan Subramanian Xiaoqing Ellen Tan Binh Tang\\nRoss Taylor Adina Williams Jian Xiang Kuan Puxin Xu Zheng Yan Iliyan Zarov Yuchen Zhang\\nAngela Fan Melanie Kambadur Sharan Narang Aurelien Rodriguez Robert Stojnic\\nSergey Edunov Thomas Scialom‚àó\\nGenAI, Meta\\nAbstract\\nIn this work, we develop and release Llama 2, a collection of pretrained and fine-tuned\\nlarge language models (LLMs) ranging in scale from 7 billion to 70 billion parameters.\\nOur fine-tuned LLMs, calledLlama 2-Chat, are optimized for dialogue use cases. Our\\nmodels outperform open-source chat models on most benchmarks we tested, and based on\\nour human evaluations for helpfulness and safety, may be a suitable substitute for closed-\\nsource models. We provide a detailed description of our approach to fine-tuning and safety\\nimprovements ofLlama 2-Chatin order to enable the community to build on our work and\\ncontribute to the responsible development of LLMs.\\n‚àóEqual contribution, corresponding authors: {tscialom, htouvron}@meta.com\\n‚Ä†Second author\\nContributions for all the authors can be found in Section A.1.\\narXiv:2307.09288v2  [cs.CL]  19 Jul 2023'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\djadh\\\\Downloads\\\\document_portal\\\\notebook for experiments\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 1, 'page_label': '2'}, page_content='Contents\\n1 Introduction 3\\n2 Pretraining 5\\n2.1 Pretraining Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2.2 Training Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2.3 Llama 2Pretrained Model Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\\n3 Fine-tuning 8\\n3.1 Supervised Fine-Tuning (SFT) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n3.2 Reinforcement Learning with Human Feedback (RLHF) . . . . . . . . . . . . . . . . . . . . . 9\\n3.3 System Message for Multi-Turn Consistency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n3.4 RLHF Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\\n4 Safety 20\\n4.1 Safety in Pretraining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\n4.2 Safety Fine-Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\\n4.3 Red Teaming . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\\n4.4 Safety Evaluation of Llama 2-Chat . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\\n5 Discussion 32\\n5.1 Learnings and Observations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\\n5.2 Limitations and Ethical Considerations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\\n5.3 Responsible Release Strategy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\\n6 Related Work 35\\n7 Conclusion 36\\nA Appendix 46\\nA.1 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\\nA.2 Additional Details for Pretraining . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\\nA.3 Additional Details for Fine-tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\\nA.4 Additional Details for Safety . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\\nA.5 Data Annotation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72\\nA.6 Dataset Contamination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75\\nA.7 Model Card . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\djadh\\\\Downloads\\\\document_portal\\\\notebook for experiments\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 2, 'page_label': '3'}, page_content='Figure 1: Helpfulness human evaluationresults forLlama\\n2-Chat compared to other open-source and closed-source\\nmodels. Human raters compared model generations on ~4k\\nprompts consisting of both single and multi-turn prompts.\\nThe95%confidenceintervalsforthisevaluationarebetween\\n1% and 2%. More details in Section 3.4.2. While reviewing\\nthese results, it is important to note that human evaluations\\ncanbenoisyduetolimitationsofthepromptset,subjectivity\\nof the review guidelines, subjectivity of individual raters,\\nand the inherent difficulty of comparing generations.\\nFigure 2: Win-rate % for helpfulness and\\nsafety between commercial-licensed base-\\nlines andLlama 2-Chat, according to GPT-\\n4. To complement the human evaluation, we\\nused a more capable model, not subject to\\nour own guidance. Green area indicates our\\nmodelisbetteraccordingtoGPT-4. Toremove\\nties, we usedwin/(win + loss). The orders in\\nwhich the model responses are presented to\\nGPT-4arerandomlyswappedtoalleviatebias.\\n1 Introduction\\nLarge Language Models (LLMs) have shown great promise as highly capable AI assistants that excel in\\ncomplex reasoning tasks requiring expert knowledge across a wide range of fields, including in specialized\\ndomains such as programming and creative writing. They enable interaction with humans through intuitive\\nchat interfaces, which has led to rapid and widespread adoption among the general public.\\nThe capabilities of LLMs are remarkable considering the seemingly straightforward nature of the training\\nmethodology. Auto-regressive transformers are pretrained on an extensive corpus of self-supervised data,\\nfollowed by alignment with human preferences via techniques such as Reinforcement Learning with Human\\nFeedback (RLHF). Although the training methodology is simple, high computational requirements have\\nlimited the development of LLMs to a few players. There have been public releases of pretrained LLMs\\n(such as BLOOM (Scao et al., 2022), LLaMa-1 (Touvron et al., 2023), and Falcon (Penedo et al., 2023)) that\\nmatch the performance of closed pretrained competitors like GPT-3 (Brown et al., 2020) and Chinchilla\\n(Hoffmann et al., 2022), but none of these models are suitable substitutes for closed ‚Äúproduct‚Äù LLMs, such\\nas ChatGPT, BARD, and Claude. These closed product LLMs are heavily fine-tuned to align with human\\npreferences, which greatly enhances their usability and safety. This step can require significant costs in\\ncompute and human annotation, and is often not transparent or easily reproducible, limiting progress within\\nthe community to advance AI alignment research.\\nIn this work, we develop and release Llama 2, a family of pretrained and fine-tuned LLMs,Llama 2and\\nLlama 2-Chat, at scales up to 70B parameters. On the series of helpfulness and safety benchmarks we tested,\\nLlama 2-Chatmodels generally perform better than existing open-source models. They also appear to\\nbe on par with some of the closed-source models, at least on the human evaluations we performed (see\\nFigures 1 and 3). We have taken measures to increase the safety of these models, using safety-specific data\\nannotation and tuning, as well as conducting red-teaming and employing iterative evaluations. Additionally,\\nthis paper contributes a thorough description of our fine-tuning methodology and approach to improving\\nLLM safety. We hope that this openness will enable the community to reproduce fine-tuned LLMs and\\ncontinue to improve the safety of those models, paving the way for more responsible development of LLMs.\\nWe also share novel observations we made during the development ofLlama 2andLlama 2-Chat, such as\\nthe emergence of tool usage and temporal organization of knowledge.\\n3')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbda9f90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## this loading have made this data into a type of document\n",
    "## 1 document = 1 page of pdf\n",
    "\n",
    "len(documents) ## 77 oages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0490a91",
   "metadata": {},
   "source": [
    "we have 77 pages , we can directly do the embedding of this or if you feel theres too much text on 1 single page and we cant preserve context or maybe context lenght of llm is not so good then you can do the **chunking**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0afc02",
   "metadata": {},
   "source": [
    "note this step is not applicable for img and autdio data , only text , that too not mandatory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "357ad029",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 500,\n",
    "    chunk_overlap = 150 , \n",
    "    length_function = len\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ca7a5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chuncked_docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d55daa9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "765"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chuncked_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e68b3d",
   "metadata": {},
   "source": [
    "to fetch metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb277e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'pdfTeX-1.40.25',\n",
       " 'creator': 'LaTeX with hyperref',\n",
       " 'creationdate': '2023-07-20T00:30:36+00:00',\n",
       " 'author': '',\n",
       " 'keywords': '',\n",
       " 'moddate': '2023-07-20T00:30:36+00:00',\n",
       " 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5',\n",
       " 'subject': '',\n",
       " 'title': '',\n",
       " 'trapped': '/False',\n",
       " 'source': 'c:\\\\Users\\\\djadh\\\\Downloads\\\\document_portal\\\\notebook for experiments\\\\data\\\\sample.pdf',\n",
       " 'total_pages': 77,\n",
       " 'page': 0,\n",
       " 'page_label': '1'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chuncked_docs[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db0893a",
   "metadata": {},
   "source": [
    "to fetch the main text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58a9c62c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Llama 2: Open Foundation and Fine-Tuned Chat Models\\nHugo Touvron‚àó Louis Martin‚Ä† Kevin Stone‚Ä†\\nPeter Albert Amjad Almahairi Yasmine Babaei Nikolay Bashlykov Soumya Batra\\nPrajjwal Bhargava Shruti Bhosale Dan Bikel Lukas Blecher Cristian Canton Ferrer Moya Chen\\nGuillem Cucurull David Esiobu Jude Fernandes Jeremy Fu Wenyin Fu Brian Fuller\\nCynthia Gao Vedanuj Goswami Naman Goyal Anthony Hartshorn Saghar Hosseini Rui Hou\\nHakan Inan Marcin Kardas Viktor Kerkez Madian Khabsa Isabel Kloumann Artem Korenev'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chuncked_docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbdb72d",
   "metadata": {},
   "source": [
    "Storing data in vector db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056fc19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f00712",
   "metadata": {},
   "source": [
    "embedding the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39b77013",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = GoogleGenerativeAIEmbeddings(model = \"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9d97c11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db = FAISS.from_documents(chuncked_docs ,\n",
    "            embedding_model\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79b2b75",
   "metadata": {},
   "source": [
    "below is the retrival process\n",
    "\n",
    "\n",
    "we will fetch / rank / retrieve the most relevant / appropriate documents from vector db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "045f640c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is pretraining of data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe7dbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_docs = vector_db.similarity_search(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d78c79f",
   "metadata": {},
   "source": [
    "fetch the most closest vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df12110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'knowledge and dampen hallucinations.\\nWe performed a variety of pretraining data investigations so that users can better understand the potential\\ncapabilities and limitations of our models; results can be found in Section 4.1.\\n2.2 Training Details\\nWe adopt most of the pretraining setting and model architecture fromLlama 1. We use the standard\\ntransformer architecture (Vaswani et al., 2017), apply pre-normalization using RMSNorm (Zhang and'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e6d080",
   "metadata": {},
   "source": [
    "we can also give hyper param of top k results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fb1456b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_k_relevant_docs = vector_db.similarity_search(query , k = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "81d514fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='0d935c9f-9171-498a-9249-d9fa861c1d98', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\djadh\\\\Downloads\\\\document_portal\\\\notebook for experiments\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 4, 'page_label': '5'}, page_content='knowledge and dampen hallucinations.\\nWe performed a variety of pretraining data investigations so that users can better understand the potential\\ncapabilities and limitations of our models; results can be found in Section 4.1.\\n2.2 Training Details\\nWe adopt most of the pretraining setting and model architecture fromLlama 1. We use the standard\\ntransformer architecture (Vaswani et al., 2017), apply pre-normalization using RMSNorm (Zhang and'),\n",
       " Document(id='d602ac04-ac75-48cc-902b-ac6e0c9f0dbd', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2023-07-20T00:30:36+00:00', 'author': '', 'keywords': '', 'moddate': '2023-07-20T00:30:36+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'c:\\\\Users\\\\djadh\\\\Downloads\\\\document_portal\\\\notebook for experiments\\\\data\\\\sample.pdf', 'total_pages': 77, 'page': 19, 'page_label': '20'}, page_content='of Llama 2-Chat(Section 4.4). We also share a model card in the Appendix, in Table 52.\\n4.1 Safety in Pretraining\\nIt is important to understand what is in the pretraining data both to increase transparency and to shed\\nlight on root causes of potential downstream issues, such as potential biases. This can inform what, if any,\\ndownstream mitigations to consider, and help guide appropriate model use. In this section, we analyze the')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_k_relevant_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe79bb6",
   "metadata": {},
   "source": [
    "what we did here was \n",
    "\n",
    "in memory storage as of now  - its stored in our RAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a392dc",
   "metadata": {},
   "source": [
    "================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b6ae7e",
   "metadata": {},
   "source": [
    "lets do it in\n",
    "\n",
    "on disk / persistent storage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a5793d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
